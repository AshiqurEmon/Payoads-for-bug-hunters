cat pingip.txt |grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b"  #from ping request to file to grep ip address
cat gospider.txt|grep -o -E "(([a-zA-Z][a-zA-Z0-9+-.]*\:\/\/)|mailto|data\:)([a-zA-Z0-9\.\&\/\?\:@\+-\_=#%;,])*" | sort -u | tee justurls.txt
cat *|rg -o "(([a-zA-Z][a-zA-Z0-9+-.]*\:\/\/)|mailto|data\:)([a-zA-Z0-9\.\&\/\?\:@\+-\_=#%;,])*"|sort -u |tee -a justurl.txt
cat *|rg -o '\b(?:https?|ftp):\/\/[^\s<>\"]+'| sed 's/]$//'|sort -u | tee -a justturls.txt


cat justurls.txt |qsreplace FUZZ|grep FUZZ|egrep -vi 'facebook|google|twitter|linkedin|youtube|vimeo|github\.com'

cat sitemap.txt |cut -d '/' -f4|grep site|anew|tee -a sitemappath.txt

2nd order subdomain grep
cat alive.txt| grep -Eo '([^.]+\.[^.]+\.[^.]+)$'|anew|tee -a alive2.txt

echo "https://hootsuite-online-revenue.s3.amazonaws.com" | grep -o 'https://\([^\.]*\)\.s3\.amazonaws\.com' | sed 's|https://\([^\.]*\)\.s3\.amazonaws\.com|\1|'
echo "https://s3.amazonaws.com/inma-awards-uploads/blashblah" | grep -o 'https://s3\.amazonaws\.com/\([^/]*\)' | sed 's|https://s3\.amazonaws\.com/\([^/]*\)|\1|'

cat subdomains.txt | awk -F'.' '{ NF--;OFS="." }1'   //exclude top level domain

cat endpoint_extractor.txt|grep -oP '(?<=://)[^/]+/\K[^?]+(?=\?|$)'|anew 

cat remove_first_slash_hash.txt| sed 's/#//g'| sed 's#^/##'| sed 's#^\./##'

echo 'testphp.vulnweb.com'|waybackurls|unfurl -u paths | sed -r 's#/#\n#g' | sort -u  //path from webarchive
echo 'testphp.vulnweb.com'|waybackurls|unfurl -u paths |anew
cat waymore_endpoint.txt|awk -F '/' '{for(i=1; i<=NF; i++) print $i}'|grep -v '\.'|anew // first two endpoint word by word


grep -oE '(http://)?[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(\.[a-zA-Z]{2,})?'   //grep subdomain from linkfinder

awk -F. '{n=NF;if($(n-1)"."$n~/^(co\.uk|com\.au|org\.uk|gov\.uk)$/){print $(n-2)"."$(n-1)"."$n}else{print $(n-1)"."$n}}'  //grep only domain part from subdomain
grep -oP '\* FUZZ: \K.*' input.txt //ffuf 

2. merge path and url
awk 'NR==FNR{subdomains[++i]=$0; next} {for (j=1; j<=i; j++) print subdomains[j] $0}' httpx.txt path.txt |anew combined.txt

3. grep -F -f urls.txt rg_justurls.txt|tee -a rg_justurls_urls.txt

cat s.txt|grep -Eo '([a-zA-Z0-9_-]+\.)+[a-zA-Z]{2,}'|grep -vE '^([a-zA-Z0-9_-]+\.[a-zA-Z]{2,})$'  //grep subdomain and exclude main domain
4 
4.1 first directory of url 
grep -oP 'https?://[^/]+/[^/.]{2,}'|egrep -vi '\?|\=|de$|fr$|text$|en$|nl$'|httpx -mc 200 |anew firsdir.txt // https://sub.dom.tld/path from https://sub.dom.tld/path/js.php

4.2 first file
grep -oP 'https?://[^/]+/[^/]+\.(php|asp|aspx|jsp|jspx)' |egrep -vi '\?|\='|httpx -mc 200|anew file.txt

4.3 first dir and file
|grep -oP 'https?://[^/]+/[^/]+/[^/]+\.(php|asp|jsp|aspx|jspx)'|egrep -v '\?|httpx -mc 200|\='|anew dirfile.txt //   https://sub.tld/dir/file.php

5. echo '/api/ws-convey/visitor-id' | awk -F'/' '{print $3}' //grep ws-convey
echo '/api/ws-convey/visitor-id' | awk -F'/' '{print $4}'    // grep visitor-id

cat s.txt|grep -Eo '([a-zA-Z0-9_-]+\.)+[a-zA-Z]{2,}'|grep -vE '^([a-zA-Z0-9_-]+\.[a-zA-Z]{2,})$'|anew sub.txt //grep subdomain exclude main domain

find . -type f | sed 's|^\./|https://sub.domain.com/|'|anew path.txt 

echo "/x/fc4fc2c/" | sed 's|^/||' //remove first slash

| grep -o '.*/static' //grep till /static

/***********
grep -irE 'word' 2>/dev/null |grep -oP '(https?://[^\s\]\)"'\''<>,]+)' //it greps urls from path
grep -E 'emon1101([^0-9]|$)' //for finding exact integer 
************\
